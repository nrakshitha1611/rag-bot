{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe807510-aea1-42df-82f3-8e88e79f2b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to your AI-powered assistant!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide the PowerPoint file path to analyze:  C:\\Users\\yatid\\Desktop\\Rakshitha\\Documents\\Fall 2024\\SVV\\DeskTest.pptx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the content from your slides...\n",
      "Building the database for content search...\n",
      "Setting up the vector database with embeddings...\n",
      "WARNING:tensorflow:From C:\\Users\\yatid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Configuring the intelligent question-answer system...\n",
      "System is ready! Ask your questions below (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your query:  Objective of the project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking...\n",
      "Querying Groq's LLM for a response...\n",
      "Answer: The objective of the project is to automate the testing of user interfaces (GUIs) for non-web desktop applications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pptx import Presentation\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from groq import Groq, APIError\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configuration constants\n",
    "GROQ_API_KEY = \"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\"\n",
    "MAX_INPUT_TOKENS = 3000\n",
    "MAX_OUTPUT_TOKENS = 500\n",
    "\n",
    "def shorten_text(text, token_limit):\n",
    "    \"\"\"Trim text to meet token constraints.\"\"\"\n",
    "    if len(text.split()) > token_limit:\n",
    "        return \" \".join(text.split()[:token_limit]) + \" [Text truncated]\"\n",
    "    return text\n",
    "\n",
    "def fetch_ppt_content(file_path):\n",
    "    \"\"\"Extracts textual content from a PowerPoint file.\"\"\"\n",
    "    ppt = Presentation(file_path)\n",
    "    all_text = []\n",
    "    for slide in ppt.slides:\n",
    "        for element in slide.shapes:\n",
    "            if element.has_text_frame:\n",
    "                all_text.append(element.text)\n",
    "    return \" \".join(all_text)\n",
    "\n",
    "def initialize_vector_database(content):\n",
    "    \"\"\"Initialize ChromaDB with HuggingFace embeddings.\"\"\"\n",
    "    try:\n",
    "        print(\"Setting up the vector database with embeddings...\")\n",
    "        embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        vector_database = Chroma.from_texts(texts=[content], embedding=embedder)\n",
    "        return vector_database\n",
    "    except Exception as ex:\n",
    "        print(f\"Error while setting up the vector database: {ex}\")\n",
    "        raise\n",
    "\n",
    "def configure_qa_system(vector_database):\n",
    "    \"\"\"Create a QA system using Groq API and the vector database.\"\"\"\n",
    "    retriever = vector_database.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "    def groq_response_engine(prompt, **kwargs):\n",
    "        \"\"\"Interact with Groq's LLM API.\"\"\"\n",
    "        try:\n",
    "            if not isinstance(prompt, str):\n",
    "                prompt = str(prompt)\n",
    "            prompt = shorten_text(prompt, MAX_INPUT_TOKENS)\n",
    "\n",
    "            print(\"Querying Groq's LLM for a response...\")\n",
    "            client = Groq(api_key=GROQ_API_KEY)\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama3-8b-8192\",\n",
    "                max_tokens=MAX_OUTPUT_TOKENS,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            # Debug the response structure\n",
    "            # print(f\"Groq API Response: {response}\")\n",
    "\n",
    "            # Extract the generated response correctly\n",
    "            return response.choices[0].message.content\n",
    "        except APIError as api_err:\n",
    "            print(f\"Groq API encountered an error: {api_err}\")\n",
    "            return \"Unable to process your request. Please try again later.\"\n",
    "        except Exception as gen_err:\n",
    "            print(f\"Unexpected error in Groq's LLM interaction: {gen_err}\")\n",
    "            return \"An error occurred while generating a response.\"\n",
    "\n",
    "    # Wrap the Groq engine in a Runnable\n",
    "    qa_pipeline = RunnableLambda(groq_response_engine)\n",
    "    return RetrievalQA.from_chain_type(llm=qa_pipeline, retriever=retriever)\n",
    "\n",
    "def ask_questions():\n",
    "    \"\"\"Interact with the user for questions and answers.\"\"\"\n",
    "    print(\"Welcome to your AI-powered assistant!\")\n",
    "    file_path = input(\"Provide the PowerPoint file path to analyze: \").strip()\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Oops! The file was not found. Please double-check the path.\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting the content from your slides...\")\n",
    "    content = fetch_ppt_content(file_path)\n",
    "\n",
    "    print(\"Building the database for content search...\")\n",
    "    try:\n",
    "        vector_db = initialize_vector_database(content)\n",
    "    except Exception as setup_err:\n",
    "        print(f\"Failed to initialize the system: {setup_err}\")\n",
    "        return\n",
    "\n",
    "    print(\"Configuring the intelligent question-answer system...\")\n",
    "    try:\n",
    "        qa_system = configure_qa_system(vector_db)\n",
    "    except Exception as config_err:\n",
    "        print(f\"Failed to configure the QA system: {config_err}\")\n",
    "        return\n",
    "\n",
    "    print(\"System is ready! Ask your questions below (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nYour query: \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye! Thanks for using this assistant.\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(\"Thinking...\")\n",
    "            answer = qa_system.run(user_input)\n",
    "            print(f\"Answer: {answer}\")\n",
    "        except APIError as api_err:\n",
    "            print(f\"Encountered a problem while retrieving the response: {api_err}\")\n",
    "        except Exception as gen_err:\n",
    "            print(f\"Unexpected error: {gen_err}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ask_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afea16-6736-4203-bb03-4c600e681fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
